{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter setup\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nasa_url = \"https://mars.nasa.gov/news/\"\n",
    "nasa_response = requests.get(nasa_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0'})\n",
    "# creating bs object\n",
    "nasa_soup = bs(nasa_response.text,'html.parser')\n",
    "\n",
    "# NOTE: For whatever reason, the bs object generated here is not catching \n",
    "# information beyond Nov. 19th entry; even adding a User-Agent argument\n",
    "# (see https://stackoverflow.com/a/26741895) doesn't work.\n",
    "# See below for proof.\n",
    "\n",
    "all_news_titles_list = []\n",
    "all_news_titles = nasa_soup.find_all(\"div\", class_=\"content_title\")\n",
    "for title in all_news_titles:\n",
    "    all_news_titles_list.append(title.text.strip())\n",
    "all_news_titles_list\n",
    "\n",
    "all_news_p_list = []\n",
    "all_news_p = nasa_soup.find_all(\"div\", class_=\"rollover_description_inner\")\n",
    "for p in all_news_p:\n",
    "    all_news_p_list.append(p.text.strip())\n",
    "all_news_p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = nasa_soup.find(\"div\", class_=\"content_title\").find(\"a\").text.strip()\n",
    "news_title\n",
    "\n",
    "news_p = nasa_soup.find(\"div\", class_=\"rollover_description_inner\").text.strip()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPL Mars Space Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<splinter.driver.webdriver.WebDriverElement object at 0x000000B47A1AC3C8>\n"
     ]
    }
   ],
   "source": [
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(jpl_url)\n",
    "html = browser.html\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "sleep(5)\n",
    "browser.click_link_by_partial_text('more info')\n",
    "sleep(5)\n",
    "img_links = browser.find_link_by_partial_href('/spaceimages')[0]\n",
    "browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = browser.find_by_css(\".main_image\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/largesize/PIA22893_hires.jpg'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs(temp.outer_html,\"lxml\").select(\"img\")[0][\"src\"]\n",
    "# need to append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note sure why it's returning these and not link text\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD SOLUTION FOR POSTERITY'S SAKE\n",
    "# See why: https://en.wikipedia.org/wiki/XMLHttpRequest\n",
    "\n",
    "# NOTE: For whatever reason, the bs object generated here is not catching \n",
    "# information beyond Nov. 19th entry; even adding a User-Agent argument\n",
    "# (see https://stackoverflow.com/a/26741895) doesn't work.\n",
    "# See below for proof.\n",
    "\n",
    "# all_news_titles_list = []\n",
    "# all_news_titles = nasa_soup.find_all(\"div\", class_=\"content_title\")\n",
    "# for title in all_news_titles:\n",
    "#     all_news_titles_list.append(title.text.strip())\n",
    "# all_news_titles_list\n",
    "\n",
    "# all_news_p_list = []\n",
    "# all_news_p = nasa_soup.find_all(\"div\", class_=\"rollover_description_inner\")\n",
    "# for p in all_news_p:\n",
    "#     all_news_p_list.append(p.text.strip())\n",
    "# all_news_p_list\n",
    "\n",
    "# news_title = nasa_soup.find(\"div\", class_=\"content_title\").find(\"a\").text.strip()\n",
    "# news_title\n",
    "\n",
    "# news_p = nasa_soup.find(\"div\", class_=\"rollover_description_inner\").text.strip()\n",
    "# news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "weather_response = requests.get(weather_url)\n",
    "weather_soup = bs(weather_response.text,'html.parser')\n",
    "\n",
    "weather_scraports = []\n",
    "weather_reports = weather_soup.find_all(\"div\", class_=\"js-tweet-text-container\")\n",
    "for report in weather_reports:\n",
    "    weather_scraports.append(report.find(\"p\", class_=\"tweet-text\").text.strip())\n",
    "weather_scraports\n",
    "\n",
    "# for scraport in weather_scraports:\n",
    "\n",
    "\n",
    "# trying to get first instance where there is the first Sol string (along with the other keywords below) \n",
    "# and then return that whole string, from the list; all in a nested if structure?\n",
    "weather_scraports.index('Sol ') \n",
    "\n",
    "# or weather_scraports.index('high ') or weather_scraports.index('low ') or weather_scraports.index('pressure ') or weather_scraports.index('hPa') or weather_scraports.index('daylight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_url = \"https://space-facts.com/mars/\"\n",
    "fact_table_extract = pd.read_html(facts_url)\n",
    "fact_table = fact_table_extract[0]\n",
    "fact_table.columns = ['Fact', 'Value']\n",
    "fact_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hemispheres_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "# browser.visit(hemispheres_url)\n",
    "# html = browser.html\n",
    "# hemispheres_soup = bs(html, 'html.parser')\n",
    "\n",
    "# make a list out of the four div class=\"item\", a.text, a.href\n",
    "\n",
    "\n",
    "hi_res_hemis_urls = []\n",
    "hemisphere_image_divs = hemispheres_soup.find_all(\"div\", class_=\"description\")\n",
    "hemisphere_image_divs\n",
    "\n",
    "# how to extract all a.text and a.href from here?\n",
    "\n",
    "# grab the names from h3 \n",
    "\n",
    "hemisphere_image_urls = hemisphere_image_divs.find_all(\"a\")\n",
    "for url in hemisphere_image_urls:\n",
    "    hi_res_hemis_urls.append(url.get(\"href\"))\n",
    "hi_res_hemis_urls\n",
    "\n",
    "\n",
    "# print(hemispheres_soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "hi_res_hemis_names =[]\n",
    "\n",
    "\n",
    "\n",
    "# need to make a list of dictionaries\n",
    "# use zip \n",
    "# https://stackoverflow.com/questions/14150797/converting-two-lists-to-a-list-of-dictionaries-in-python\n",
    "\n",
    "\n",
    "hi_res_hemis = [\n",
    "    # {\"title\": \" \", \"img_url\": \" \"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop\n",
    "# browser.back()\n",
    "\n",
    "# scrape craigslist exercise for reference \n",
    "\n",
    "# make soup first\n",
    "# store in a list the div items \n",
    "# for link in links_list that I'm gonna try to make\n",
    "\n",
    "# div class= items, there are 4 on the page\n",
    "# partial text click with \"enhanced\"\n",
    "# get link for link with \"original\"\n",
    "# with find link by text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
